## 1. Cross-Validation
- Training a model on specific data and testing it on the same data is a methodological error; it would have perfect accuracy but would fail to perf0orm on different or unknown data. This is referred to as Overfitting. 
- To avoid this, data should be separated into training data and testing data. 
- When evaluating different hyperparameters there is still a risk of overfitting on the test set because parameters can be twitched to perform optimally. 
- This can leak the knowledge about test cases into the model and can overfit. 
- To solve this problem another part of the dataset can be created as a “ Validation set ”.
- So training is done on training data, evaluation is done on validation data and for final evaluation testing data should be used.
- However, splitting data into three sets (Training set, Testing set, and validation set) reduces the quantity of data required to train the model. 
- A solution to this problem is called “Cross-Validation”. 
- Cross-validation is a technique used to validate the Machine Learning model and select the best model according to its performance. 
- A test set is still needed but there is no use of a validation set. 

## There are plenty of Cross-Validation techniques, Let’s see some of them:


## Hold-out:- 

- Hold-out cross-validation is a simple validation technique and is most commonly used. The algorithm of Hold-out can be given as:
- Divide the dataset into two sets i.e training and testing set, Mostly 80% of data is used for training and 20% for testing.
- Validate on the test set.


## K-Fold:- 

K-Fold is a cross-validation technique that helps to evaluate the model many times, unlike hold-out where we can test the model only once. The algorithm of K-Fold is as follows:
- Choose the number of folds K usually it is 5 or 10 split the dataset into K equal partition.
- Pick (K-1) fold as training data and the remaining will be used as test data.
- train the model on training data, for each iteration of the CV a new model is trained independently of others.
- Test model on the test data.
- Repeat the previous 3 steps k time and get a cross-validation score by averaging all the k scores.

To perform k-Fold cross-validation the sklearn module can be used as follows.


import numpy as np

from sklearn.model_selection import KFold

    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])

    y = np.array([1, 2, 3, 4])

    kf = KFold(n_splits=2,shuffle=False)
 
     for train_index, test_index in kf.split(X):

     print("TRAIN:", train_index, "TEST:", test_index)
     
     X_train, X_test = X[train_index], X[test_index]
     
     y_train, y_test = y[train_index], y[test_index]



- In the above code we have imported the required module, created a dataset as X(features) and y(labels), next we have created an object of k-Fold. 
- The parameters of the KFold method means : n_splits: Number of folds (Must be at least 2). 
- shuffle: Whether to shuffle the data before splitting to batches. 
- K-Fold also has a disadvantage if k is given a higher value, training model becomes expensive and time-consuming.


## Stratified k-Fold

- When we have an imbalance type of data, For example, we have a dataset to predict handwritten digits and we have a large number of 3-digit categories compared to other digits there might be a large shift toward 3-digit. 
- To overcome this problem we use the Stratified k-Fold method it is a variation of k-Fold. 
- Stratified k-Fold splits the data into k sets where each set contains the same percentage of sample for each target category. 
- In regression, it makes sure that the mean of each fold is the same. 

The stratified K-Fold algorithm is as follows :

-Choose the number of folds K usually it is 5 or 10 split the dataset into K equal partition where each fold has the same percentage of sample for each category.
- Pick (K-1) fold as training data and the remaining will be used as test data.
- Train the model on training data, for each iteration of the CV a new model is trained independently of others.
- Test model on the test data.
- Repeat the previous 3 steps k time and get a cross-validation score by averaging all the k scores.


As you can see it is the same as K-Fold cross-validation, just the data splitting part is different. To perform Stratified k-Fold cross-validation you can use the sklearn module as follows.


    import numpy as np
    from sklearn.model_selection import StratifiedKFold
    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8],[9,10],[11,12]])
    y = np.array([0, 0, 0, 1,0,1])
    skf = StratifiedKFold(n_splits=2)
    for train_index, test_index in skf.split(X, y):
    print("Train:", train_index, "Test:", test_index)
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]



- Above we can see the splitting of the data set is done in such a way that each category(“0” and “1”) is present in both train and test data set. 
- The output indicates the index of data set, [ 2,4,5 ] → [0,0,1 ] (value at index of y) [ 0,1,3 ] → [0,0,1 ] (value at index of y) 
- Here we can see that category in y is distributed equally to both the sets even though we have more “0” categories than “1”.
