##  üìö Handwritten notes with proper implementation of each algorithm from scratch 
 ‚úî KNN 
 - In depth theory with example.
 
 ‚úî Linear Regression
-  Indepth intution of maths required of Linear Regression
-  Derivation of Gradient Descent for Univariate Linear Regression
-  Derivation of Gradient Descent for Multivariate Linear Regression
-  Optimzation Algorithm - Batch Gradient Descent, stochastic GD, Mini Batch GD
-  Closed Form solution derivation 
-  Locally Weighted Regression Derivation 
-  Maximum Likelihood Estimation for Linear Regression

‚úî Logitstic Regression 
- Hypothesis function
-  Log Loss
-  Proof of Log loss by MLE
-  Gradient Descent Update rule for Logistic Regression

‚úî Feature Selection and Extraction
- Feature Selection - Chi2 test, randomforest classifier
- Feature Extraction - Principal Component Analysis 

‚úîÔ∏è Naive Bayes
- Bayes Theorem Formula 
- Bayes Theorem - Spam or not
- Bayes Theorem - Disease or not
- Mushroom Classification
- Text Classification
- Laplace Smoothing
- Multivariate Bernoulli Naive Bayes
- Multivariate Event Model Naive Bayes
-  Multivariate Bernoulli Naive Bayes vs Multivariate Event Model Naive Bayes
-  Gaussian Naive Bayes

‚úîÔ∏è Decision Tree and Random Forest Classifier
- Entropy
- Information Gain
- Process Kaggle Titanic Dataset 
- Implementation of Information Gain
- Implementation of Decision Tree
- Making Predictions
- Decision Trees using Sci-kit Learn
- Random Forest Ensembles
